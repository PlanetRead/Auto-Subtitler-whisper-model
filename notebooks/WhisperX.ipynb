{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'alignment (Python 3.10.16)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n alignment ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import whisperx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def transcribe_with_whisperx(audio_file, model_name=\"large-v2\", language_code=\"pa\"):\n",
    "    \"\"\"\n",
    "    Transcribe and align an audio file using WhisperX.\n",
    "    \n",
    "    Args:\n",
    "      audio_file (str): Path to your audio (e.g., .wav, .mp3).\n",
    "      model_name (str): Whisper model variant, e.g., \"tiny\", \"base\", \"medium\", \"large-v2\".\n",
    "      language_code (str): Language code for alignment model (e.g., \"en\", \"hi\", \"es\").\n",
    "                          Check WhisperX docs for supported codes.\n",
    "                          \n",
    "    Returns:\n",
    "      A dictionary containing segment-level and word-level alignment data.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 1) Load the main Whisper model\n",
    "    print(f\"Loading Whisper model {model_name} on {device}...\")\n",
    "    model = whisperx.load_model(model_name, device=device)\n",
    "\n",
    "    # 2) Transcribe the audio\n",
    "    print(f\"Transcribing {audio_file}...\")\n",
    "    audio = whisperx.load_audio(audio_file)\n",
    "    # batch_size=16 is an example; adjust as needed for your GPU/CPU capabilities\n",
    "    transcription_result = model.transcribe(audio, batch_size=16)\n",
    "\n",
    "    # transcription_result is a dict with keys like [\"text\", \"segments\", \"language\"]\n",
    "    # Each segment is {\"start\": float, \"end\": float, \"text\": str, ...}\n",
    "\n",
    "    # 3) Load alignment model\n",
    "    #    Make sure the language_code is supported by the alignment model in WhisperX\n",
    "    print(f\"Loading alignment model for language={language_code}...\")\n",
    "    alignment_model, metadata = whisperx.load_align_model(\n",
    "        language_code=language_code,  # e.g., \"hi\" if you're aligning Hindi or similar\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 4) Align the transcribed segments to get word-level timestamps\n",
    "    print(f\"Aligning results...\")\n",
    "    aligned_result = whisperx.align(\n",
    "        transcription_result[\"segments\"],  # the segments from transcription\n",
    "        alignment_model,\n",
    "        metadata,\n",
    "        audio,\n",
    "        device=device\n",
    "    )\n",
    "    # aligned_result is also a dict with \"segments\" that include \"words\" arrays\n",
    "\n",
    "    return {\n",
    "        \"language\": transcription_result[\"language\"],\n",
    "        \"segments\": aligned_result[\"segments\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model tiny on cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ecd7b05e494c9ca377560684e6cccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fc8b8f7ed748978304e12accf342d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52d7adc3aea4dec909eb55089f6d681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873795e99fd8457291358b0f649a7eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/75.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../miniconda3/envs/alignment/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n",
      "Transcribing /home/arjun/naren/subtitle_alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/miniconda3/envs/alignment/lib/python3.10/site-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    AUDIO_FILE =  \"/home/arjun/naren/subtitle_alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi.wav\"\n",
    "    # If trying Punjabi, you might pass language_code=\"hi\" or \"en\" to see if alignment is possible.\n",
    "    # Officially, WhisperX might not have a dedicated 'pa' alignment model yet.\n",
    "    \n",
    "    # For best results, pick a code that is closest to or partially supports your language. \n",
    "    # Or skip alignment if your language is not supported (only segment-level times).\n",
    "    language_code_for_alignment = \"hi\"\n",
    "\n",
    "    result = transcribe_with_whisperx(\n",
    "        AUDIO_FILE, \n",
    "        model_name=\"tiny\", \n",
    "        language_code=language_code_for_alignment\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Transcription & Alignment Results ===\")\n",
    "    print(f\"Detected language: {result['language']}\")\n",
    "    print(\"Segments (with word-level timestamps):\")\n",
    "    for seg in result[\"segments\"]:\n",
    "        seg_start = seg[\"start\"]\n",
    "        seg_end = seg[\"end\"]\n",
    "        seg_text = seg[\"text\"]\n",
    "        print(f\"[{seg_start:.2f}s - {seg_end:.2f}s]: {seg_text}\")\n",
    "\n",
    "        # Word-level details\n",
    "        if \"words\" in seg:\n",
    "            for w in seg[\"words\"]:\n",
    "                w_start = w[\"start\"]\n",
    "                w_end = w[\"end\"]\n",
    "                w_text = w[\"word\"]\n",
    "                print(f\"    -> {w_text} [{w_start:.2f}s - {w_end:.2f}s]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
