{"cells":[{"cell_type":"markdown","metadata":{"id":"uYE_iuB545nx"},"source":["# Whisper Inference for Punjabi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":775,"status":"ok","timestamp":1735703557400,"user":{"displayName":"Narendiran C G","userId":"01294287589978941220"},"user_tz":-330},"id":"_LGPCm-iUBKV","outputId":"d1dc176d-20af-45d1-c5a8-51909586bd74"},"outputs":[],"source":["# Check GPU availability\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":366,"status":"ok","timestamp":1735705051050,"user":{"displayName":"Narendiran C G","userId":"01294287589978941220"},"user_tz":-330},"id":"oJivbuahFRpE"},"outputs":[],"source":["# All the imports\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","from copy import deepcopy\n","\n","from datasets import DatasetDict, Dataset, Audio\n","import soundfile as sf\n","import pysrt"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["data_path = Path(\"../data/Punjabi\")\n","audio_path = data_path / \"Audio\"\n","transcript_path = data_path / \"Text\"\n","srt_path = data_path / \"Ground Truth SRT\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["benchmark_df = pd.read_csv(data_path / \"benchmark_list.csv\", index_col=0).reset_index(drop=True)\n","benchmark_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["benchmark_paths = benchmark_df[\"Story Name\"].unique().tolist()\n","benchmark_paths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create lists to store decoded audio and transcripts\n","base_lists = {\"audio_path\": [], \"transcript_path\": [], \"srt_path\": []}\n","dataset = {\"train\": deepcopy(base_lists), \"val\": deepcopy(base_lists)}\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1191,"status":"ok","timestamp":1734899884649,"user":{"displayName":"Arjun Jain","userId":"05491890224946824197"},"user_tz":-330},"id":"s3EMaOOW6R5A","outputId":"4fe2ed2b-a4e0-4d53-b0aa-48c4bf156c3e"},"outputs":[],"source":["for dir_path in audio_path.glob(\"*\"):\n","    for file_path in dir_path.glob(\"*.wav\"):\n","        print(file_path.name)\n","        if any(benchmark_path in file_path.name for benchmark_path in benchmark_paths):\n","            dataset_type = \"val\"\n","        else:\n","            dataset_type = \"train\"\n","        print(dataset_type)\n","\n","        # Append audio path and transcript path to dataset\n","        file_audio_path = str(file_path)\n","        file_transcript_path = transcript_path / dir_path.name.replace(\"Videos\", \"Text\") / str(file_path.name.replace(\".wav\", \".txt\"))\n","        file_srt_path = srt_path / dir_path.name.replace(\"Videos\", \"SRT\") / str(file_path.name.replace(\".wav\", \".srt\"))\n","        dataset[dataset_type][\"audio_path\"].append(file_audio_path)\n","        dataset[dataset_type][\"transcript_path\"].append(file_transcript_path)\n","        dataset[dataset_type][\"srt_path\"].append(file_srt_path)  \n","        \n","        # # Load and decode audio\n","        # decoded_audio, sampling_rate = sf.read(file_audio_path)\n","        # dataset[dataset_type][\"array\"].append(decoded_audio)\n","        # dataset[dataset_type][\"sampling_rate\"].append(sampling_rate)\n","\n","        # # Read transcript and append to dataset\n","        # with open(file_transcript_path, 'r', encoding='utf-8') as file:\n","        #     transcript = file.read()\n","        #     dataset[dataset_type][\"transcript\"].append(transcript)"]},{"cell_type":"markdown","metadata":{},"source":["## Create Whisper Feature Extractor, Tokenizer and Processor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3243,"status":"ok","timestamp":1734899887890,"user":{"displayName":"Arjun Jain","userId":"05491890224946824197"},"user_tz":-330},"id":"nWOmsmqEUBKb","outputId":"3585f394-433c-44c7-8ce7-d32167983121"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v2\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"CtvWWzA8UBKc"},"outputs":[],"source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v2\", language=\"Punjabi\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"H1EyPMZmUBKd"},"outputs":[],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", language=\"Punjabi\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"H1cUNr_xUBKf"},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","import torch\n","import librosa\n","\n","\n","# Load Whisper model and processor\n","model_name = \"openai/whisper-large-v2\"\n","model = WhisperForConditionalGeneration.from_pretrained(model_name).eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import torchaudio\n","\n","def load_audio(file_path):\n","    speech_array, sampling_rate = torchaudio.load(file_path)\n","    if sampling_rate != 16000:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n","        speech_array = resampler(speech_array)\n","    return speech_array.squeeze(), 16000"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Load and preprocess audio\n","audio_path = \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi.wav\"\n","import os\n","import math\n","from pydub import AudioSegment\n","\n","def chunk_audio(file_path, chunk_length_ms=30000):\n","    audio = AudioSegment.from_wav(file_path)\n","    audio_length_ms = len(audio)\n","    chunks = []\n","    for i in range(0, audio_length_ms, chunk_length_ms):\n","        chunk = audio[i:i + chunk_length_ms]\n","        chunks.append(chunk)\n","    return chunks\n","\n","def save_chunks(chunks, base_path):\n","    for i, chunk in enumerate(chunks):\n","        chunk_name = f\"{base_path}/chunk{i}.wav\"\n","        chunk.export(chunk_name, format=\"wav\")\n","\n","# Chunk the audio file into 30-second segments\n","chunks = chunk_audio(audio_path, chunk_length_ms=30000)\n","save_chunks(chunks, \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks\")"]},{"cell_type":"markdown","metadata":{},"source":["## Transcription"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["audio_chunk_path = \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\"\n","\n","speech, _ = load_audio(audio_chunk_path)\n","input_features = processor(speech, sampling_rate=16000, return_tensors=\"pt\").input_features.to(model.device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"punjabi\", task=\"transcribe\")\n","\n","# Generate transcription with timestamps\n","with torch.no_grad():\n","    generated_ids = model.generate(input_features, return_timestamps=False)\n","\n","# Decode and clean special tokens\n","transcription = processor.batch_decode(generated_ids, output_word_offsets=True)\n","# cleaned_transcription = transcription[0].replace(\"<|startoftranscript|>\", \"\").replace(\"<|endoftranscript|>\", \"\").strip()\n","\n","# print(\"Transcription:\", cleaned_transcription)\n","print(transcription)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"punjabi\", task=\"transcribe\")\n","\n","# Generate transcription with timestamps\n","with torch.no_grad():\n","    generated_ids = model.generate(input_features, return_timestamps=True)\n","\n","# Decode and clean special tokens\n","transcription = processor.batch_decode(generated_ids, output_word_offsets=False, max_length=448)\n","# cleaned_transcription = transcription[0].replace(\"<|startoftranscript|>\", \"\").replace(\"<|endoftranscript|>\", \"\").strip()\n","\n","# print(\"Transcription:\", cleaned_transcription)\n","print(transcription)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define your initial prompt\n","initial_prompt = \"ਸਤ ਸ੍ਰੀ ਅਕਾਲ, ਇਹ ਇੱਕ ਪੰਜਾਬੀ ਆਡੀਓ ਫਾਇਲ ਹੈ, ਇਸ ਦਾ ਸਮੱਗਰੀ ਹੇਠ ਲਿਖੀ ਹੈ۔\"\n","forced_decoder_ids = processor.get_decoder_prompt_ids(text=initial_prompt, language=\"pa\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import pipeline\n","\n","# Load whisper-large-v3\n","model_name = \"openai/whisper-large-v2\"\n","whisper_pipeline = pipeline(\n","    task=\"automatic-speech-recognition\", \n","    model=model_name, \n","    model_kwargs={\"forced_decoder_ids\": forced_decoder_ids},  \n","    device=0  # Specify GPU device\n",")\n","\n","# Transcribe audio with timestamps\n","output = whisper_pipeline(\n","    \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\",\n","    return_timestamps=True,\n","    chunk_length_s=10,  # Chunk size in seconds\n","    stride_length_s=1,  # Overlapping stride to prevent cutoff\n",")\n","\n","# Extract timestamps and transcriptions\n","for segment in output[\"chunks\"]:\n","    print(f\"Text: {segment['text']} | Start: {segment['timestamp'][0]}s | End: {segment['timestamp'][1]}s\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq\n","\n","# Load the Whisper model and processor\n","model_name = \"openai/whisper-large-v2\"\n","processor = AutoProcessor.from_pretrained(model_name)\n","model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n","\n","# Define the initial prompt\n","initial_prompt = \"ਸਤ ਸ੍ਰੀ ਅਕਾਲ, ਇਹ ਇੱਕ ਪੰਜਾਬੀ ਆਡੀਓ ਫਾਇਲ ਹੈ, ਇਸ ਦਾ ਸਮੱਗਰੀ ਹੇਠ ਲਿਖੀ ਹੈ۔\"\n","\n","# Encode the initial prompt manually using the tokenizer\n","tokenizer = processor.tokenizer\n","forced_decoder_ids = tokenizer.encode(initial_prompt, add_special_tokens=False, return_tensors=\"pt\").squeeze(0).tolist()\n","\n","# Create the pipeline with the model and forced_decoder_ids\n","whisper_pipeline = pipeline(\n","    task=\"automatic-speech-recognition\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    feature_extractor=processor.feature_extractor,\n","    model_kwargs={\"forced_decoder_ids\": forced_decoder_ids},  # Pass the encoded initial prompt\n","    device=0,  # Use GPU if available\n",")\n","\n","# Transcribe audio with the pipeline\n","output = whisper_pipeline(\n","    \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\",\n","    return_timestamps=True,\n","    chunk_length_s=5,  # Specify chunk size in seconds\n","    stride_length_s=1,  # Specify overlapping stride\n",")\n","\n","# Extract and print timestamps and transcriptions\n","for segment in output[\"chunks\"]:\n","    print(f\"Text: {segment['text']} | Start: {segment['timestamp'][0]}s | End: {segment['timestamp'][1]}s\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output[\"chunks\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load audio\n","audio_path = \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\"\n","audio, sr = librosa.load(audio_path, sr=16000)\n","\n","# Encode input\n","inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True).to(\"cuda\")\n","\n","# Enable timestamps\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"pa\", task=\"translate\")\n","print(forced_decoder_ids)\n","# Generate output\n","with torch.no_grad():\n","    generated_ids = model.generate(\n","        inputs.input_features,\n","        forced_decoder_ids=forced_decoder_ids,  # Ensure no `<|notimestamps|>` token\n","        return_timestamps=True,\n","        #output_word_offsets=True\n","    )\n","\n","# Decode output\n","transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","print(transcription)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Split the string wherever there is `><`\n","segments = transcription[0].split('><')\n","\n","# Print each segment\n","for i in range(len(segments)):\n","    if  i < len(segments) - 1:\n","        segments[i] = segments[i] + '>'\n","\n","    if i > 0:\n","        segments[i] = '<' + segments[i]\n","print(segments)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","# Load the IndicTrans model and tokenizer\n","# model_name = \"ai4bharat/indic-bert\"\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","# Load model directly\n","from transformers import AutoModel\n","model = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")\n","\n","\n","# Function to extract text and timestamps\n","def extract_text_and_timestamps(segment):\n","    parts = segment.split('>')\n","    timestamp_start = parts[0] + '>'\n","    text = parts[1].rsplit('<', 1)[0]\n","    timestamp_end = '<' + parts[1].rsplit('<', 1)[1]\n","    return timestamp_start, text, timestamp_end\n","\n","# Generate Punjabi transcript for the English text and preserve timestamps\n","punjabi_transcripts = []\n","for segment in segments:\n","    timestamp_start, text, timestamp_end = extract_text_and_timestamps(segment)\n","    # Tokenize the input text\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    # Generate translation\n","    translated_tokens = model.generate(**inputs)\n","    # Decode the translated tokens\n","    punjabi_transcript = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n","\n","    punjabi_transcripts.append(f\"{timestamp_start}{punjabi_transcript}{timestamp_end}\")\n","\n","# Print the Punjabi transcripts with timestamps\n","for transcript in punjabi_transcripts:\n","    print(transcript)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load audio\n","audio_path = \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\"\n","audio, sr = librosa.load(audio_path, sr=16000)\n","\n","# Encode input\n","inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True).to(\"cuda\")\n","\n","# Enable timestamps\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"punjabi\", task=\"transcribe\")\n","\n","\n","# Generate output\n","with torch.no_grad():\n","    generated = model.generate(\n","        inputs.input_features,\n","        forced_decoder_ids=forced_decoder_ids,\n","        return_timestamps=True,  # Ensuring timestamps are returned\n","    )\n","\n","# Decode output with timestamps\n","transcription = processor.batch_decode(generated, skip_special_tokens=False)\n","  \n","# Print transcription with timestamps\n","for i, segment in enumerate(transcription):\n","    print(f\"Segment {i}: {segment}\")\n","\n","# Now let's extract the timestamps from the output\n","# `generated` will contain a dictionary with 'sequences' and 'timestamps'\n","timestamps = generated['timestamps']\n","print(f\"Timestamps: {timestamps}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Pipeline code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import pipeline\n","\n","# Replace with the path to your Punjabi audio file\n","AUDIO_FILE = \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\"\n","\n","# Load a Whisper large model (v2) from Hugging Face\n","# Note: There's no \"v3\" on the Hub, so we use the latest large-v2 model.\n","# If you have a custom \"v3\" model, just replace with its Hugging Face repo name.\n","# whisper_asr = pipeline(\n","#     task=\"automatic-speech-recognition\",\n","#     model=\"openai/whisper-large-v2\",\n","#     chunk_length_s=30,          # Process audio in 30 second chunks\n","#     return_timestamps=True,     # Return timestamps for each chunk\n","#     generate_kwargs={\n","#         \"language\": \"<|pa|>\",   # Force the model to use Punjabi\n","#         \"task\": \"transcribe\"    # Instruct the model to perform transcription\n","#     }\n","# )\n","\n","whisper_asr = pipeline(\n","    task=\"automatic-speech-recognition\",\n","    model=\"openai/whisper-large-v2\",\n","    chunk_length_s=30,\n","    return_timestamps=True,\n",")\n","\n","# Overwrite forced_decoder_ids at the model level\n","forced_decoder_ids = whisper_asr.tokenizer.get_decoder_prompt_ids(\n","    language=\"pa\",\n","    task=\"transcribe\",\n","    no_timestamps=False\n",")\n","whisper_asr.model.generation_config.forced_decoder_ids = forced_decoder_ids\n","\n","# Run transcription\n","result = whisper_asr(AUDIO_FILE)\n","\n","# 'result' will have a 'chunks' key containing segments with timestamps\n","print(\"Transcription with timestamps:\\n\")\n","for chunk in result[\"chunks\"]:\n","    start_time, end_time = chunk[\"timestamp\"]\n","    text = chunk[\"text\"]\n","    print(f\"[{start_time:.2f}s - {end_time:.2f}s] {text}\")"]},{"cell_type":"markdown","metadata":{},"source":["## New try"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["import torch\n","import torchaudio\n","import torchaudio.transforms as T\n","import re\n","from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","\n","# ------------------------------------------------\n","# 1. Configuration\n","# ------------------------------------------------\n","AUDIO_FILE = \"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi.wav\"  # Replace with your audio path\n","MODEL_ID = \"openai/whisper-large-v2\"\n","CHUNK_LENGTH_S = 30       # 30-second chunks\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# ------------------------------------------------\n","# 2. Load Model & Processor\n","# ------------------------------------------------\n","processor = WhisperProcessor.from_pretrained(MODEL_ID)\n","model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)\n","\n","# Force the model to produce timestamps in Punjabi transcription:\n","# - no_timestamps=False ensures the model does NOT insert <|notimestamps|>.\n","forced_decoder_ids = processor.get_decoder_prompt_ids(\n","    language=\"pa\",\n","    task=\"transcribe\",\n","    no_timestamps=False      # crucial!\n",")\n"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["# ------------------------------------------------\n","# 3. Load & Resample Audio to 16 kHz\n","# ------------------------------------------------\n","audio_waveform, original_sr = torchaudio.load(AUDIO_FILE)\n","if original_sr != 16000:\n","    resampler = T.Resample(orig_freq=original_sr, new_freq=16000)\n","    audio_waveform = resampler(audio_waveform)\n","sr = 16000\n","\n","# Convert to mono if needed\n","if audio_waveform.shape[0] > 1:\n","    audio_waveform = torch.mean(audio_waveform, dim=0, keepdim=True)\n","\n","num_samples = audio_waveform.shape[1]\n","chunk_size = CHUNK_LENGTH_S * sr"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["# ------------------------------------------------\n","# 4. Chunking Helper\n","# ------------------------------------------------\n","def audio_chunks_generator(waveform, chunk_samples):\n","    \"\"\"\n","    Yields (chunk_waveform, chunk_start_sec).\n","    \"\"\"\n","    start = 0\n","    while start < num_samples:\n","        end = min(start + chunk_samples, num_samples)\n","        chunk = waveform[:, start:end]\n","        chunk_start_sec = start / sr\n","        yield chunk, chunk_start_sec\n","        start = end\n","\n","# ------------------------------------------------\n","# 5. Parsing Timestamp Tokens\n","# ------------------------------------------------\n","# We want to keep <|xx.xx|> but remove other special tokens\n","# like <|startoftranscript|>, <|pa|>, <|transcribe|>, etc.\n","\n","def clean_special_tokens(text: str) -> str:\n","    \"\"\"\n","    Remove known special tokens except those used for timestamps (<|xx.xx|>).\n","    \"\"\"\n","    # Remove <|startoftranscript|>, <|pa|>, <|transcribe|>, etc.\n","    # but do NOT remove <|0.00|> or similar tokens.\n","    # We'll do a simple approach that specifically targets known tokens.\n","    tokens_to_remove = [\n","        r\"<\\|startoftranscript\\|>\",\n","        r\"<\\|endoftext\\|>\",\n","        r\"<\\|pa\\|>\",\n","        r\"<\\|transcribe\\|>\",\n","        r\"<\\|translate\\|>\",\n","        r\"<\\|notimestamps\\|>\"\n","    ]\n","    for pattern in tokens_to_remove:\n","        text = re.sub(pattern, \"\", text)\n","    return text.strip()\n","\n","def parse_timestamped_text(text_with_ts):\n","    \"\"\"\n","    Given a text that may contain <|0.00|> tokens, split it into segments:\n","        [(time_in_s, segment_text), ...]\n","    We assume the text has already been cleaned of other tokens,\n","    so only <|xx.xx|> remains.\n","    \"\"\"\n","    # This regex will capture the numeric part inside <|xx.xx|>\n","    ts_pattern = re.compile(r\"<\\|(\\d+\\.\\d+)\\|>\")\n","\n","    segments = []\n","    current_text = \"\"\n","    current_time = 0.0\n","\n","    # Split on <|xx.xx|> but keep the time in the result\n","    parts = ts_pattern.split(text_with_ts)\n","    # Example: [\"Some text \", \"0.00\", \" more text \", \"3.24\", \" more text\", ...]\n","\n","    for i, part in enumerate(parts):\n","        if i % 2 == 0:\n","            # This is text content\n","            # Accumulate or finalize a segment\n","            text_segment = part.strip()\n","            if text_segment:\n","                segments.append((current_time, text_segment))\n","        else:\n","            # This is the timestamp (odd index)\n","            current_time = float(part)\n","\n","    return segments"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["all_segments = []\n","\n","# Process each chunk independently\n","for chunk_waveform, chunk_start_sec in get_audio_chunks(audio_waveform, chunk_size):\n","    # 1) Prepare input features\n","    input_features = processor(\n","        chunk_waveform.squeeze(0),\n","        sampling_rate=sr,\n","        return_tensors=\"pt\"\n","    ).input_features.to(DEVICE)\n","\n","    # 2) Inference\n","    with torch.no_grad():\n","        generated_tokens = model.generate(\n","            input_features,\n","            forced_decoder_ids=forced_decoder_ids,\n","            max_length=448,   # you can adjust\n","            # You can set other generation parameters here\n","        )\n","   \n","    # 3) Decode *with* special tokens so we can see <|xx.xx|>\n","    raw_transcription = processor.batch_decode(generated_tokens, skip_special_tokens=False)[0]\n","    \n","    # 4) Clean out all special tokens EXCEPT <|xx.xx|>\n","    cleaned_text = clean_special_tokens(raw_transcription)\n","\n","    # 5) Parse the cleaned text to extract (timestamp, text)\n","    segments_in_chunk = parse_timestamped_text(cleaned_text)\n","\n","    # 6) Offset each timestamp by the chunk start\n","    for rel_t, seg_text in segments_in_chunk:\n","        abs_t = chunk_start_sec + rel_t\n","        all_segments.append((abs_t, seg_text))\n","\n","# ------------------------------------------------\n","# 7. Print Final Results\n","# ------------------------------------------------\n","print(\"Transcription with approximate timestamps:\\n\")\n","for t, txt in all_segments:\n","    print(f\"[{t:.2f}s] {txt}\")"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["# ------------------------------------------------\n","# 6. Perform Transcription with Timestamps\n","# ------------------------------------------------\n","all_segments = []\n","\n","for chunk_waveform, chunk_start_sec in audio_chunks_generator(audio_waveform, chunk_size):\n","    # 1) Convert waveform to input features\n","    inputs = processor(\n","        chunk_waveform.squeeze(0),\n","        sampling_rate=sr,\n","        return_tensors=\"pt\"\n","    )\n","\n","    input_features = inputs.input_features.to(DEVICE)\n","\n","    # 2) Generate tokens (no timestamps skipping)\n","    with torch.no_grad():\n","        generated_tokens = model.generate(\n","            input_features,\n","            forced_decoder_ids=forced_decoder_ids,\n","            max_length=448,  # can adjust\n","            # You can set other decoding params: temperature, top_k, etc.\n","        )\n","\n","    # 3) Decode *with* special tokens so we can see <|xx.xx|>\n","    raw_transcription = processor.batch_decode(generated_tokens, skip_special_tokens=False)[0]\n","    \n","    # 4) Clean out all special tokens EXCEPT <|xx.xx|>\n","    cleaned_text = clean_special_tokens(raw_transcription)\n","\n","    # 5) Parse the cleaned text to extract (timestamp, text)\n","    segments_in_chunk = parse_timestamped_text(cleaned_text)\n","\n","    # 6) Offset each timestamp by the chunk start\n","    for rel_t, seg_text in segments_in_chunk:\n","        abs_t = chunk_start_sec + rel_t\n","        all_segments.append((abs_t, seg_text))\n","\n","# ------------------------------------------------\n","# 7. Print Final Results\n","# ------------------------------------------------\n","print(\"Transcription with approximate timestamps:\\n\")\n","for t, txt in all_segments:\n","    print(f\"[{t:.2f}s] {txt}\")"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["import transformers\n","transformers.__version__"]},{"cell_type":"markdown","metadata":{},"source":["## Post processing"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def format_timestamp(seconds):\n","    \"\"\"Convert seconds to HH:MM:SS format\"\"\"\n","    hours = int(seconds // 3600)\n","    minutes = int((seconds % 3600) // 60)\n","    seconds = seconds % 60\n","    return f\"{hours:02d}:{minutes:02d}:{seconds:05.2f}\"\n","\n","# Process the output to get clean timestamps and text\n","chunks = []\n","current_chunk = {\"text\": \"\", \"start\": 0, \"end\": 0}\n","\n","# Split the transcription into chunks with timestamps\n","for chunk in transcription[0].split(\"<|\"):\n","    if \">\" in chunk:\n","        timestamp_info = chunk.split(\">\")[0]\n","        if timestamp_info.startswith(\"0.00\"):\n","            current_chunk[\"start\"] = 0.00\n","        elif \"timestamp\" in timestamp_info:\n","            time = float(timestamp_info.split(\"]\")[0].split(\"[\")[-1])\n","            if current_chunk[\"text\"]:\n","                current_chunk[\"end\"] = time\n","                chunks.append(current_chunk.copy())\n","            current_chunk = {\"text\": \"\", \"start\": time, \"end\": 0}\n","    else:\n","        current_chunk[\"text\"] += chunk.strip()\n","\n","# Format the output\n","formatted_transcript = []\n","for chunk in chunks:\n","    if chunk[\"text\"].strip():  # Only include non-empty chunks\n","        formatted_transcript.append({\n","            \"text\": chunk[\"text\"].strip(),\n","            \"start\": format_timestamp(chunk[\"start\"]),\n","            \"end\": format_timestamp(chunk[\"end\"]),\n","            \"start_seconds\": chunk[\"start\"],\n","            \"end_seconds\": chunk[\"end\"]\n","        })\n","\n","\n","# Print formatted output\n","for segment in formatted_transcript:\n","    print(f\"[{segment['start']} --> {segment['end']}] {segment['text']}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Whisper library"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install openai-whisper\n","!sudo apt-get install ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFDarvj2_5Fc"},"outputs":[],"source":["import whisper\n","# Load the Whisper model\n","model = whisper.load_model(\"medium\")\n","\n","# Transcribe and translate the chunk audio of Punjabi\n","result = model.transcribe(\"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\", task=\"translate\")\n","\n","# Print the transcription and translation result\n","print(result[\"text\"])\n","# !pip install scipy --no-cache-dir"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_H-z8SE-A7p"},"outputs":[],"source":["# Load the Whisper model\n","model = whisper.load_model(\"turbo\")\n","\n","# Transcribe and translate the chunk audio of Punjabi\n","result = model.transcribe(\"/home/arjun/naren/alignment/data/Punjabi/Audio/AniBook Videos/Abdul_Kalam,_Missile_Man_Punjabi_chunks/chunk0.wav\", task=\"translate\")\n","\n","# Print the transcription and translation result\n","print(result[\"text\"])\n","\n","\n","# !pip install --upgrade scipy\n","# !pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE_LE_93UBKf"},"outputs":[],"source":["model.generation_config.language = \"punjabi\"\n","model.generation_config.task = \"transcribe\"\n","\n","model.generation_config.forced_decoder_ids = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8BvGWVAUBKg"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","    decoder_start_token_id: int\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\", max_length=self.processor.tokenizer.model_max_length)\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPGx66LkUBKg"},"outputs":[],"source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n","    processor=processor,\n","    decoder_start_token_id=model.config.decoder_start_token_id,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5n8EgSeUBKg"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"wer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqnZU1TdUBKg"},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    print(\"Predictions: \\n\", pred_str)\n","    print(\"Labels: \\n\", label_str)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1734899900578,"user":{"displayName":"Arjun Jain","userId":"05491890224946824197"},"user_tz":-330},"id":"Wvn7JtA-UBKg","outputId":"d6d2ac88-513e-44c7-9ee1-5d35861e297c"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"whisper-large-v3-pa\",  # change to a repo name of your choice\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-4,\n","    warmup_steps=50,\n","    max_steps=100,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=50,\n","    eval_steps=10,\n","    logging_steps=10,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1734900415209,"user":{"displayName":"Arjun Jain","userId":"05491890224946824197"},"user_tz":-330},"id":"lWXrfAC5UBKh","outputId":"0f4613f3-1133-4a42-96e5-e4cc458a1d7f"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"train\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":308330,"status":"ok","timestamp":1734900725895,"user":{"displayName":"Arjun Jain","userId":"05491890224946824197"},"user_tz":-330},"id":"GQKjqPLaUBKh","outputId":"27368aa3-1313-431e-8756-371ea91bb481"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1734900215005,"user":{"displayName":"Arjun Jain","userId":"05491890224946824197"},"user_tz":-330},"id":"TfnU1Gtavson","outputId":"768ca98d-a71b-4709-e866-91fce45ca543"},"outputs":[],"source":["import torch\n","import gc\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1734900293641,"user":{"displayName":"Arjun Jain","userId":"05491890224946824197"},"user_tz":-330},"id":"tLC3c7RDEgQI","outputId":"1fbc46f5-5318-4a7f-e9cb-4ebacb50caa3"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlqKEEquvson"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1Bdiwk8-cnH3nJBD6ENDcZfwFmrgZDtrM","timestamp":1735705079208}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3a64fdddd0624bc094455c516e5cacef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3eb8ab26a45b48b39585faed17039664":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4087853866024331a9a065fbdb722c2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41fd39d7ffc44620925de7683092419a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f33c9f02c3a42fd98e1660cc095eae8","placeholder":"​","style":"IPY_MODEL_649fb3b81fb64780b64f7706f18fbb78","value":"Map: 100%"}},"4f33c9f02c3a42fd98e1660cc095eae8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"649fb3b81fb64780b64f7706f18fbb78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68546b13fe7e4ae2b17ce18907ab6e64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f722f8fe8b524e289bf9a36eee23f263","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87a1abe581f048aaa02be13795f3e8d4","value":2}},"87a1abe581f048aaa02be13795f3e8d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6744e5c5ec14cd6a10dd32480ec25d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41fd39d7ffc44620925de7683092419a","IPY_MODEL_68546b13fe7e4ae2b17ce18907ab6e64","IPY_MODEL_d78bf484c902455bb0c6e47017e81e3a"],"layout":"IPY_MODEL_3eb8ab26a45b48b39585faed17039664"}},"d78bf484c902455bb0c6e47017e81e3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4087853866024331a9a065fbdb722c2d","placeholder":"​","style":"IPY_MODEL_3a64fdddd0624bc094455c516e5cacef","value":" 2/2 [00:05&lt;00:00,  2.34s/ examples]"}},"f722f8fe8b524e289bf9a36eee23f263":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
