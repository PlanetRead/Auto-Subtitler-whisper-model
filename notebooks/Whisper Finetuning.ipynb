{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYE_iuB545nx"
   },
   "source": [
    "# Finetuning Whisper for Punjabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1735705051050,
     "user": {
      "displayName": "Narendiran C G",
      "userId": "01294287589978941220"
     },
     "user_tz": -330
    },
    "id": "oJivbuahFRpE"
   },
   "outputs": [],
   "source": [
    "# All the imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "import soundfile as sf\n",
    "import pysrt\n",
    "import json\n",
    "import jiwer\n",
    "\n",
    "from typing import Dict, Any, Optional\n",
    "import logging\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/Punjabi\")\n",
    "audio_path = data_path / \"Audio\"\n",
    "transcript_path = data_path / \"Text\"\n",
    "srt_path = data_path / \"Ground Truth SRT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df = pd.read_csv(data_path / \"benchmark_list.csv\", index_col=0).reset_index(drop=True)\n",
    "benchmark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_names = benchmark_df[\"Story Name\"].unique().tolist()\n",
    "benchmark_categories = benchmark_df[\"Category\"].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store decoded audio and transcripts\n",
    "base_lists = {\"audio_path\": [], \"transcript_path\": [], \"srt_path\": []}\n",
    "dataset = {\"train\": deepcopy(base_lists), \"val\": deepcopy(base_lists)}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1191,
     "status": "ok",
     "timestamp": 1734899884649,
     "user": {
      "displayName": "Arjun Jain",
      "userId": "05491890224946824197"
     },
     "user_tz": -330
    },
    "id": "s3EMaOOW6R5A",
    "outputId": "4fe2ed2b-a4e0-4d53-b0aa-48c4bf156c3e"
   },
   "outputs": [],
   "source": [
    "for dir_path in audio_path.glob(\"*\"):\n",
    "    for file_path in dir_path.glob(\"*.wav\"):\n",
    "        print(file_path.name)\n",
    "        if any(benchmark_path in file_path.name for benchmark_path in benchmark_names):\n",
    "            dataset_type = \"val\"\n",
    "        else:\n",
    "            dataset_type = \"train\"\n",
    "        print(dataset_type)\n",
    "\n",
    "        # Append audio path and transcript path to dataset\n",
    "        file_audio_path = str(file_path)\n",
    "        file_transcript_path = transcript_path / dir_path.name.replace(\"Videos\", \"Text\") / str(file_path.name.replace(\".wav\", \".txt\"))\n",
    "        file_srt_path = srt_path / dir_path.name.replace(\"Videos\", \"SRT\") / str(file_path.name.replace(\".wav\", \".srt\"))\n",
    "        dataset[dataset_type][\"audio_path\"].append(file_audio_path)\n",
    "        dataset[dataset_type][\"transcript_path\"].append(file_transcript_path)\n",
    "        dataset[dataset_type][\"srt_path\"].append(file_srt_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_srt_start_end(start, end):\n",
    "    \"\"\"Convert SRT timestamp objects to seconds.\n",
    "    \n",
    "    Args:\n",
    "        start: pysrt.SubRipTime object for start time\n",
    "        end: pysrt.SubRipTime object for end time\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (start_seconds, end_seconds)\n",
    "    \"\"\"\n",
    "    def time_to_seconds(t):\n",
    "        return t.hours * 3600 + t.minutes * 60 + t.seconds + t.milliseconds / 1000\n",
    "        \n",
    "    return time_to_seconds(start), time_to_seconds(end)\n",
    "\n",
    "# def combine_subtitles(srt_data, max_chunk_duration=15.0):\n",
    "#     \"\"\"Combine consecutive subtitle chunks into logical sentences.\n",
    "    \n",
    "#     Combines subtitles while:\n",
    "#     1. Respecting maximum duration limits\n",
    "#     2. Splitting on sentence boundaries (punctuation marks)\n",
    "#     3. Cleaning up formatting and unwanted symbols\n",
    "    \n",
    "#     Args:\n",
    "#         srt_data: List of pysrt.SubRipItem objects\n",
    "#         max_chunk_duration: Maximum duration in seconds for combined chunks\n",
    "    \n",
    "#     Returns:\n",
    "#         List of combined pysrt.SubRipItem objects\n",
    "#     \"\"\"\n",
    "#     if not srt_data or max_chunk_duration is None:\n",
    "#         return srt_data\n",
    "        \n",
    "#     # Constants\n",
    "#     PUNCTUATION_MARKS = {'।', '!', '?', '॥', '...', '...!', '♪'}\n",
    "#     QUOTE_REPLACEMENTS = {\n",
    "#         '“': '\"',\n",
    "#         '”': '\"', \n",
    "#         '‘': \"'\",\n",
    "#         '’': \"'\",\n",
    "#         '…': '...',\n",
    "#         '-': ''\n",
    "#     }\n",
    "    \n",
    "#     def clean_subtitle_text(text):\n",
    "#         \"\"\"Clean and normalize subtitle text.\"\"\"\n",
    "#         # Remove text in square brackets\n",
    "#         text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        \n",
    "#         # Replace quotes and other symbols\n",
    "#         for old, new in QUOTE_REPLACEMENTS.items():\n",
    "#             text = text.replace(old, new)\n",
    "            \n",
    "#         # Remove leading/trailing ♪ and whitespace\n",
    "#         text = text.strip('♪').strip()\n",
    "        \n",
    "#         # Normalize whitespace\n",
    "#         return ' '.join(text.split())\n",
    "    \n",
    "#     def should_split_chunk(text):\n",
    "#         \"\"\"Check if chunk should be split based on punctuation.\"\"\"\n",
    "#         return any(text.endswith(mark) for mark in PUNCTUATION_MARKS)\n",
    "    \n",
    "#     combined_srt_data = []\n",
    "#     current_chunk = None\n",
    "    \n",
    "#     for subtitle in srt_data:\n",
    "#         subtitle_text = clean_subtitle_text(subtitle.text)\n",
    "        \n",
    "#         if not subtitle_text:  # Skip empty subtitles\n",
    "#             continue\n",
    "            \n",
    "#         if current_chunk is None:\n",
    "#             current_chunk = subtitle\n",
    "#             current_chunk.text = subtitle_text\n",
    "#             continue\n",
    "            \n",
    "#         # Check if combining would exceed duration limit\n",
    "#         start_sec, end_sec = get_srt_start_end(current_chunk.start, subtitle.end)\n",
    "#         if end_sec - start_sec <= max_chunk_duration:\n",
    "#             # Combine chunks\n",
    "#             current_chunk.end = subtitle.end\n",
    "#             current_chunk.text = f\"{current_chunk.text} {subtitle_text}\"\n",
    "            \n",
    "#             if should_split_chunk(current_chunk.text):\n",
    "#                 combined_srt_data.append(current_chunk)\n",
    "#                 current_chunk = None\n",
    "#         else:\n",
    "#             # Duration would be too long, split here\n",
    "#             combined_srt_data.append(current_chunk)\n",
    "#             current_chunk = subtitle\n",
    "#             current_chunk.text = subtitle_text\n",
    "            \n",
    "#             if should_split_chunk(current_chunk.text):\n",
    "#                 combined_srt_data.append(current_chunk)\n",
    "#                 current_chunk = None\n",
    "    \n",
    "#     # Add final chunk if exists\n",
    "#     if current_chunk is not None:\n",
    "#         combined_srt_data.append(current_chunk)\n",
    "        \n",
    "#     return combined_srt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_srt_time_str(time_obj, adjust=False):\n",
    "    \"\"\"Convert SRT timestamp object to string format with 0.02s resolution.\n",
    "    \n",
    "    Args:\n",
    "        time_obj: pysrt.SubRipTime object\n",
    "        \n",
    "    Returns:\n",
    "        str: Time in seconds with 0.02s resolution (e.g. \"10.50\")\n",
    "    \"\"\"\n",
    "    seconds = (time_obj.hours * 3600 + \n",
    "              time_obj.minutes * 60 + \n",
    "              time_obj.seconds + \n",
    "              time_obj.milliseconds / 1000)\n",
    "    # Round to nearest 0.02 seconds\n",
    "    rounded_seconds = round(seconds * 50) / 50\n",
    "    if adjust:\n",
    "        rounded_seconds -= 0.02\n",
    "    return f\"{rounded_seconds:.2f}\"\n",
    "\n",
    "def combine_subtitles(srt_data, max_chunk_duration=15.0):\n",
    "    \"\"\"Combine consecutive subtitle chunks with timestamps.\n",
    "    \n",
    "    Combines subtitles while:\n",
    "    1. Respecting maximum duration limits\n",
    "    2. Adding timestamps in format <|seconds|>\n",
    "    3. Cleaning up formatting and unwanted symbols\n",
    "    \n",
    "    Args:\n",
    "        srt_data: List of pysrt.SubRipItem objects\n",
    "        max_chunk_duration: Maximum duration in seconds for combined chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of combined pysrt.SubRipItem objects with timestamps in text\n",
    "    \"\"\"\n",
    "    if not srt_data or max_chunk_duration is None:\n",
    "        return srt_data\n",
    "        \n",
    "    # Constants\n",
    "    QUOTE_REPLACEMENTS = {\n",
    "        '“': '\"',\n",
    "        '”': '\"', \n",
    "        '‘': \"'\",\n",
    "        '’': \"'\",\n",
    "        '…': '...',\n",
    "        '-': ''\n",
    "    }\n",
    "    \n",
    "    def clean_subtitle_text(text):\n",
    "        \"\"\"Clean and normalize subtitle text.\"\"\"\n",
    "        # Remove text in square brackets\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        \n",
    "        # Replace quotes and other symbols\n",
    "        for old, new in QUOTE_REPLACEMENTS.items():\n",
    "            text = text.replace(old, new)\n",
    "            \n",
    "        # Remove leading/trailing ♪ and whitespace\n",
    "        text = text.strip('♪').strip()\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    combined_srt_data = []\n",
    "    current_chunk = None\n",
    "    chunk_start_time = 0.0\n",
    "    \n",
    "    for subtitle in srt_data:\n",
    "        subtitle_text = clean_subtitle_text(subtitle.text)\n",
    "        \n",
    "        if not subtitle_text:  # Skip empty subtitles\n",
    "            continue\n",
    "            \n",
    "        # Calculate relative timestamps\n",
    "        if current_chunk is None:\n",
    "            chunk_start_time = float(get_srt_time_str(subtitle.start))\n",
    "            relative_start = 0.0\n",
    "        else:\n",
    "            relative_start = float(get_srt_time_str(subtitle.start)) - chunk_start_time\n",
    "            \n",
    "        relative_end = float(get_srt_time_str(subtitle.end, adjust=True)) - chunk_start_time\n",
    "        \n",
    "        # Add timestamps to text\n",
    "        timestamped_text = (f\"<|{relative_start:.2f}|> \"\n",
    "                          f\"{subtitle_text} \"\n",
    "                          f\"<|{relative_end:.2f}|>\")\n",
    "        \n",
    "        if current_chunk is None:\n",
    "            current_chunk = subtitle\n",
    "            current_chunk.text = timestamped_text\n",
    "            continue\n",
    "            \n",
    "        # Check if combining would exceed duration limit\n",
    "        if relative_end <= max_chunk_duration:\n",
    "            # Combine chunks\n",
    "            current_chunk.end = subtitle.end\n",
    "            current_chunk.text = f\"{current_chunk.text} {timestamped_text}\"\n",
    "        else:\n",
    "            # Duration would be too long, split here\n",
    "            combined_srt_data.append(current_chunk)\n",
    "            current_chunk = subtitle\n",
    "            # Reset timestamps for new chunk\n",
    "            chunk_start_time = float(get_srt_time_str(subtitle.start))\n",
    "            current_chunk.text = f\"<|0.00|> {subtitle_text} <|{float(get_srt_time_str(subtitle.end, adjust=True)) - chunk_start_time:.2f}|>\"\n",
    "    \n",
    "    # Add final chunk if exists\n",
    "    if current_chunk is not None:\n",
    "        combined_srt_data.append(current_chunk)\n",
    "        \n",
    "    return combined_srt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_srt_start_end(start, end):\n",
    "#     \"\"\"\n",
    "#     Get the start and end time in seconds from a pysrt.SubRipItem object\n",
    "#     \"\"\"\n",
    "#     start_sec = (start.hours * 3600 + \n",
    "#                 start.minutes * 60 + \n",
    "#                 start.seconds + \n",
    "#                 start.milliseconds / 1000)\n",
    "#     end_sec = (end.hours * 3600 + \n",
    "#                 end.minutes * 60 + \n",
    "#                 end.seconds + \n",
    "#                 end.milliseconds / 1000)\n",
    "#     return start_sec, end_sec\n",
    "\n",
    "\n",
    "\n",
    "# def combine_subtitles(srt_data, max_chunk_duration=15.0):\n",
    "#     \"\"\"\n",
    "#     Combine consecutive subtitle chunks such that their total duration doesn't exceed max_duration,\n",
    "#     and split whenever a logical sentence ends with specific punctuation.\n",
    "    \n",
    "#     Args:\n",
    "#         srt_data: List of pysrt.SubRipItem objects\n",
    "#         max_chunk_duration: Maximum duration in seconds for combined chunks\n",
    "    \n",
    "#     Returns:\n",
    "#         List of combined pysrt.SubRipItem objects\n",
    "#     \"\"\"\n",
    "#     if max_chunk_duration is None:\n",
    "#         return srt_data\n",
    "#     combined_srt_data = []\n",
    "#     current_chunk = None\n",
    "#     punctuation_marks = ['।', '!', '?', '॥', '...', '...!', '♪']\n",
    "\n",
    "#     for subtitle in srt_data:\n",
    "#         # Remove text within square brackets and unwanted punctuation\n",
    "#         subtitle_text = subtitle.text\n",
    "#         subtitle_text = re.sub(r'\\[.*?\\]', '', subtitle_text)  # Remove text in square brackets\n",
    "        \n",
    "#         subtitle_text = subtitle_text.replace('“', '\"').replace('”', '\"').replace('‘', \"'\").replace('’', \"'\")  # Replace quotes\n",
    "#         subtitle_text = subtitle_text.replace('…', '...').replace('-', '')  # Remove ellipsis and replace with ... and remove '-\n",
    "#         subtitle_text = subtitle_text.strip()  # Remove leading/trailing whitespace\n",
    "#         subtitle_text = \" \".join(subtitle_text.split())\n",
    "\n",
    "#         # Remove leading and trailing ♪ symbols and treat them as punctuation\n",
    "#         if subtitle_text.strip().startswith('♪'):\n",
    "#             subtitle_text = subtitle_text.strip()[1:].strip()\n",
    "\n",
    "#         # print(subtitle_text)\n",
    "\n",
    "#         if current_chunk is None:\n",
    "#             current_chunk = subtitle\n",
    "#             current_chunk.text = subtitle_text\n",
    "#         else:\n",
    "#             # Calculate duration if we combine this subtitle\n",
    "#             start_sec, end_sec = get_srt_start_end(current_chunk.start, subtitle.end)\n",
    "#             potential_duration = end_sec - start_sec\n",
    "            \n",
    "#             if potential_duration <= max_chunk_duration:\n",
    "#                 # Combine with current chunk\n",
    "#                 current_chunk.end = subtitle.end\n",
    "#                 # Handle the music symbol\n",
    "#                 current_chunk.text = current_chunk.text.strip().replace('♪', '').strip()\n",
    "#                 current_chunk.text = f\"{current_chunk.text} {subtitle_text}\"\n",
    "                \n",
    "#                 # Check if the current chunk ends with a punctuation mark\n",
    "#                 if any(current_chunk.text.endswith(mark) for mark in punctuation_marks):\n",
    "\n",
    "#                     # Handle the music symbol\n",
    "#                     current_chunk.text = current_chunk.text.strip().replace('♪', '').strip()\n",
    "#                     # Add current chunk to results and start new one\n",
    "#                     combined_srt_data.append(current_chunk)\n",
    "#                     current_chunk = None  # Reset current_chunk to start a new one\n",
    "#             else:\n",
    "#                 # If the potential duration exceeds max_chunk_duration, add the current chunk\n",
    "#                 # to results and start a new chunk with the current subtitle\n",
    "#                 # Handle the music symbol\n",
    "#                 current_chunk.text = current_chunk.text.strip().replace('♪', '').strip()\n",
    "#                 combined_srt_data.append(current_chunk)\n",
    "#                 current_chunk = subtitle\n",
    "#                 current_chunk.text = subtitle_text\n",
    "                \n",
    "#                 # Check if the new current_chunk ends with a punctuation mark\n",
    "#                 if any(current_chunk.text.endswith(mark) for mark in punctuation_marks):\n",
    "\n",
    "#                     # Handle the music symbol\n",
    "#                     current_chunk.text = current_chunk.text.strip().replace('♪', '').strip()\n",
    "#                     combined_srt_data.append(current_chunk)\n",
    "#                     current_chunk = None  # Reset current_chunk to start a new one\n",
    "\n",
    "#     # Add the last chunk if it exists\n",
    "#     if current_chunk is not None:\n",
    "#         combined_srt_data.append(current_chunk)\n",
    "    \n",
    "#     return combined_srt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_file_path = dataset[\"val\"][\"srt_path\"][1]\n",
    "print(srt_file_path)\n",
    "\n",
    "srt_data = pysrt.open(srt_file_path)\n",
    "combined_srt_data = combine_subtitles(srt_data)\n",
    "\n",
    "for i, sub in enumerate(combined_srt_data):\n",
    "    sub_duration = sub.end - sub.start\n",
    "    print(f\"chunk :{i} start: {sub.start} end: {sub.end} duration: {sub_duration}\")\n",
    "    print(sub.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dataset(\n",
    "    dataset: Dict[str, Dict[str, list]], \n",
    "    finetuning_data_path: Path,\n",
    "    min_chunk_duration: float = 1,  # Minimum chunk duration in seconds\n",
    "    max_chunk_duration: float = 15.0,  # Maximum chunk duration in seconds\n",
    "    combined_chunk_duration: Optional[float] = None,  # Maximum chunk duration in seconds\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Chunk each sentence from SRT files into separate chunks of audio and text for finetuning.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dictionary containing train/val splits with audio and SRT paths\n",
    "        finetuning_data_path: Base path to store chunked data\n",
    "        min_chunk_duration: Minimum allowed duration for audio chunks in seconds\n",
    "        max_chunk_duration: Maximum allowed duration for audio chunks in seconds\n",
    "        combined_chunk_duration: Maximum duration for combined chunks in seconds\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    for split in dataset:\n",
    "        logger.info(f\"Processing {split} split...\")\n",
    "        # Create output directories if they don't exist\n",
    "        audio_out_path = finetuning_data_path / split / \"Audio\"\n",
    "        text_out_path = finetuning_data_path / split / \"Text\"\n",
    "        audio_out_path.mkdir(parents=True, exist_ok=True)\n",
    "        text_out_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Verify required keys exist\n",
    "        if \"srt_path\" not in dataset[split] or \"audio_path\" not in dataset[split]:\n",
    "            logger.error(f\"Missing required paths in {split} split\")\n",
    "            continue\n",
    "\n",
    "        for srt_file_path, audio_file_path in zip(dataset[split][\"srt_path\"], dataset[split][\"audio_path\"]):\n",
    "            try:\n",
    "                # Load SRT file\n",
    "                srt_data = pysrt.open(srt_file_path)\n",
    "\n",
    "                # Combine subtitles into chunks of combined_chunk_duration\n",
    "                combined_srt_data = combine_subtitles(srt_data, max_chunk_duration=combined_chunk_duration)\n",
    "                \n",
    "                # Load audio file\n",
    "                audio_data, sampling_rate = sf.read(audio_file_path)\n",
    "                \n",
    "                logger.info(f\"Processing file: {Path(audio_file_path).name}\")\n",
    "                \n",
    "                for i, sentence in enumerate(combined_srt_data):\n",
    "                    try:\n",
    "                        # Convert timedelta to seconds\n",
    "                        start_sec, end_sec = get_srt_start_end(sentence.start, sentence.end)\n",
    "                        \n",
    "                        # Validate chunk duration\n",
    "                        duration = end_sec - start_sec\n",
    "                        if duration < min_chunk_duration:\n",
    "                            logger.warning(f\"Skipping chunk {i}: duration too short ({duration:.2f}s)\")\n",
    "                            continue\n",
    "                        if duration > max_chunk_duration:\n",
    "                            logger.warning(f\"Skipping chunk {i}: duration too long ({duration:.2f}s)\")\n",
    "                            continue\n",
    "                            \n",
    "                        # Convert time to samples\n",
    "                        start_sample = int(start_sec * sampling_rate)\n",
    "                        end_sample = int(end_sec * sampling_rate)\n",
    "                        \n",
    "                        # Validate sample indices\n",
    "                        if end_sample > len(audio_data):\n",
    "                            logger.warning(f\"Chunk {i} exceeds audio duration, truncating\")\n",
    "                            end_sample = len(audio_data)\n",
    "                        \n",
    "                        # Extract audio chunk\n",
    "                        audio_chunk = audio_data[start_sample:end_sample]\n",
    "                        \n",
    "                        # Generate output paths\n",
    "                        chunk_name = f\"{Path(audio_file_path).stem}_chunk{i:04d}\"\n",
    "                        audio_out_file = audio_out_path / f\"{chunk_name}.wav\"\n",
    "                        text_out_file = text_out_path / f\"{chunk_name}.txt\"\n",
    "                        \n",
    "                        # Save audio chunk and transcript\n",
    "                        sf.write(audio_out_file, audio_chunk, sampling_rate)\n",
    "                        text_out_file.write_text(\" \".join(sentence.text.strip().split()), encoding='utf-8')\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing chunk {i} of {audio_file_path.name}: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {audio_file_path}: {str(e)}\")\n",
    "                continue\n",
    "    logger.info(\"Dataset chunking completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "finetuning_data_path = Path(\"/spiral_hdd_2/workspace/naren/data/timestamped_chunks\")\n",
    "chunk_dataset(\n",
    "    dataset=dataset,\n",
    "    finetuning_data_path=finetuning_data_path,\n",
    "    min_chunk_duration=0.25,  # Minimum quarter second\n",
    "    max_chunk_duration=30.0,  # Maximum 30 seconds\n",
    "    combined_chunk_duration=30.0,  # Maximum 10 seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create finetuning dataset and create lists to store decoded audio and transcripts\n",
    "base_lists = {\"audio_path\": [], \"transcript_path\": []}\n",
    "finetuning_dataset = {\"train\": deepcopy(base_lists), \"val\": deepcopy(base_lists)}\n",
    "finetuning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_cutoff = 100\n",
    "val_cutoff = 10\n",
    "\n",
    "train_cutoff = np.inf\n",
    "val_cutoff = np.inf\n",
    "\n",
    "for split in finetuning_dataset:\n",
    "    for i, audio_path in enumerate((finetuning_data_path / split / \"Audio\").glob(\"*.wav\")):\n",
    "        if i == train_cutoff and split == \"train\":\n",
    "            break\n",
    "        if i == val_cutoff and split == \"val\":\n",
    "            break\n",
    "        transcript_path = (finetuning_data_path / split / \"Text\") / str(audio_path.name.replace(\".wav\", \".txt\"))\n",
    "        finetuning_dataset[split][\"audio_path\"].append(str(audio_path))\n",
    "        finetuning_dataset[split][\"transcript_path\"].append(str(transcript_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuning_dataset[\"train\"][\"audio_path\"])\n",
    "print(finetuning_dataset[\"train\"][\"transcript_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finetuning_dataset[\"val\"][\"audio_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finetuning_dataset[\"train\"][\"audio_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuning_dataset[\"val\"][\"audio_path\"][0])\n",
    "print(finetuning_dataset[\"val\"][\"transcript_path\"][0])\n",
    "print(finetuning_dataset[\"val\"][\"audio_path\"][1])\n",
    "print(finetuning_dataset[\"val\"][\"transcript_path\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(finetuning_dataset[\"val\"][\"transcript_path\"][0], \"r\") as f:\n",
    "    print(f.read())\n",
    "with open(finetuning_dataset[\"val\"][\"transcript_path\"][1], \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Whisper Feature Extractor, Tokenizer and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-large-v2\"\n",
    "language = \"Punjabi\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3243,
     "status": "ok",
     "timestamp": 1734899887890,
     "user": {
      "displayName": "Arjun Jain",
      "userId": "05491890224946824197"
     },
     "user_tz": -330
    },
    "id": "nWOmsmqEUBKb",
    "outputId": "3585f394-433c-44c7-8ce7-d32167983121"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CtvWWzA8UBKc"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "H1EyPMZmUBKd"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "WWeHG-ixUBKf"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # Load audio file\n",
    "    audio_array, sampling_rate = sf.read(str(batch[\"audio_path\"]))\n",
    "    \n",
    "    # Read transcript\n",
    "    with open(batch[\"transcript_path\"], 'r', encoding='utf-8') as file:\n",
    "        transcript = file.read().strip()\n",
    "    \n",
    "    # Compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0]\n",
    "    \n",
    "    # Add array and sampling rate to batch\n",
    "    batch[\"array\"] = audio_array\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    \n",
    "    # Encode target text to label ids\n",
    "    max_input_length = 448  # Maximum sequence length allowed by the model\n",
    "    batch[\"labels\"] = tokenizer(transcript, padding=True, truncation=True, max_length=max_input_length).input_ids\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_dataset = DatasetDict()\n",
    "whisper_dataset[\"train\"] = Dataset.from_dict(finetuning_dataset[\"train\"])\n",
    "whisper_dataset[\"val\"] = Dataset.from_dict(finetuning_dataset[\"val\"])\n",
    "len(whisper_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b6744e5c5ec14cd6a10dd32480ec25d7",
      "41fd39d7ffc44620925de7683092419a",
      "68546b13fe7e4ae2b17ce18907ab6e64",
      "d78bf484c902455bb0c6e47017e81e3a",
      "3eb8ab26a45b48b39585faed17039664",
      "4f33c9f02c3a42fd98e1660cc095eae8",
      "649fb3b81fb64780b64f7706f18fbb78",
      "f722f8fe8b524e289bf9a36eee23f263",
      "87a1abe581f048aaa02be13795f3e8d4",
      "4087853866024331a9a065fbdb722c2d",
      "3a64fdddd0624bc094455c516e5cacef"
     ]
    },
    "executionInfo": {
     "elapsed": 6492,
     "status": "ok",
     "timestamp": 1734899896891,
     "user": {
      "displayName": "Arjun Jain",
      "userId": "05491890224946824197"
     },
     "user_tz": -330
    },
    "id": "FtLB4KJ1UBKf",
    "outputId": "9936ea50-65cd-4457-f6e9-d3cc689524a9"
   },
   "outputs": [],
   "source": [
    "whisper_dataset[\"train\"] = whisper_dataset[\"train\"].map(prepare_dataset, num_proc=len(whisper_dataset[\"train\"]))\n",
    "whisper_dataset[\"val\"] = whisper_dataset[\"val\"].map(prepare_dataset, num_proc=len(whisper_dataset[\"val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(whisper_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_dataset_path = finetuning_data_path / \"whisper_dataset_1200_15.pkl\"\n",
    "whisper_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(whisper_dataset_path, \"wb\") as f:\n",
    "    pickle.dump(whisper_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the whisper dataset for finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(whisper_dataset_path, \"rb\") as f:\n",
    "    whisper_dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(whisper_dataset[\"train\"][0][\"transcript_path\"], \"r\") as f:\n",
    "    print(tokenizer(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Whisper Model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "H1cUNr_xUBKf"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper model and processor\n",
    "model.generation_config.language = language\n",
    "model.generation_config.task = task\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\", max_length=self.processor.tokenizer.model_max_length)\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # # Strip whitespace and clean up strings\n",
    "    # pred_str = [pred.strip() for pred in pred_str if pred.strip()]\n",
    "    # label_str = [label.strip() for label in label_str if label.strip()]\n",
    "\n",
    "    print(\"Predictions: \\n\", pred_str)\n",
    "    print(\"Labels: \\n\", label_str)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[\"Hi, how are iyou?\", \"Fine, thank youu\", \"Fine, thank youu\"], references=[\"Hi, how are you?\", \"Fine, thank you\", \"Fine, thank you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter a new checkpoints path\n",
    "checkpoints_path = \"/spiral_hdd_2/workspace/naren/data/models/whisper-large-v2-pa-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=checkpoints_path,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=10,\n",
    "    max_steps=250,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=25,\n",
    "    eval_steps=5,\n",
    "    logging_steps=5,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# # Add EarlyStoppingCallback\n",
    "# early_stopping_callback = EarlyStoppingCallback(\n",
    "#     early_stopping_patience=3  # Number of evaluations with no improvement before stopping\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=whisper_dataset[\"train\"],\n",
    "    eval_dataset=whisper_dataset[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model explicitly to a new directory\n",
    "trainer.save_model(output_dir=checkpoints_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to Whisper pt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def convert_hf_to_openai_checkpoint(hf_model, model_size=\"large-v2\"):\n",
    "    \"\"\"\n",
    "    Convert a HF Whisper model to an OpenAI Whisper-style checkpoint\n",
    "    with correct key naming for 'attn_ln', 'mlp_ln', 'cross_attn', etc.\n",
    "    \"\"\"\n",
    "\n",
    "    hf_sd = hf_model.state_dict()\n",
    "    openai_sd = OrderedDict()\n",
    "\n",
    "    def rename_key(key: str) -> str:\n",
    "        new_key = key\n",
    "\n",
    "        # 1) Remove \"model.\" prefix\n",
    "        if new_key.startswith(\"model.\"):\n",
    "            new_key = new_key.replace(\"model.\", \"\")\n",
    "\n",
    "        # 2) \"encoder.layers\" -> \"encoder.blocks\"\n",
    "        new_key = new_key.replace(\"encoder.layers\", \"encoder.blocks\")\n",
    "        #    \"decoder.layers\" -> \"decoder.blocks\"\n",
    "        new_key = new_key.replace(\"decoder.layers\", \"decoder.blocks\")\n",
    "\n",
    "        # 3) Token/pos embeddings\n",
    "        new_key = new_key.replace(\"embed_tokens.weight\", \"token_embedding.weight\")\n",
    "        new_key = new_key.replace(\"embed_positions.weight\", \"positional_embedding\")\n",
    "\n",
    "        # 4) Self-attention names\n",
    "        #    HF:  \"self_attn.{q_proj|k_proj|v_proj|out_proj}\"\n",
    "        #    OAI: \"attn.{query|key|value|out}\"\n",
    "        new_key = new_key.replace(\"self_attn.q_proj.weight\", \"attn.query.weight\")\n",
    "        new_key = new_key.replace(\"self_attn.q_proj.bias\",   \"attn.query.bias\")\n",
    "        new_key = new_key.replace(\"self_attn.k_proj.weight\", \"attn.key.weight\")\n",
    "        new_key = new_key.replace(\"self_attn.k_proj.bias\",   \"attn.key.bias\")\n",
    "        new_key = new_key.replace(\"self_attn.v_proj.weight\", \"attn.value.weight\")\n",
    "        new_key = new_key.replace(\"self_attn.v_proj.bias\",   \"attn.value.bias\")\n",
    "        new_key = new_key.replace(\"self_attn.out_proj.weight\", \"attn.out.weight\")\n",
    "        new_key = new_key.replace(\"self_attn.out_proj.bias\",   \"attn.out.bias\")\n",
    "\n",
    "        # 5) Cross-attention (decoder only):\n",
    "        #    HF:  \"encoder_attn.{q_proj|k_proj|v_proj|out_proj}\", \"encoder_attn_layer_norm\"\n",
    "        #    OAI: \"cross_attn.{query|key|value|out}\", \"cross_attn_ln\"\n",
    "        new_key = new_key.replace(\"encoder_attn.q_proj.weight\", \"cross_attn.query.weight\")\n",
    "        new_key = new_key.replace(\"encoder_attn.q_proj.bias\",   \"cross_attn.query.bias\")\n",
    "        new_key = new_key.replace(\"encoder_attn.k_proj.weight\", \"cross_attn.key.weight\")\n",
    "        new_key = new_key.replace(\"encoder_attn.k_proj.bias\",   \"cross_attn.key.bias\")\n",
    "        new_key = new_key.replace(\"encoder_attn.v_proj.weight\", \"cross_attn.value.weight\")\n",
    "        new_key = new_key.replace(\"encoder_attn.v_proj.bias\",   \"cross_attn.value.bias\")\n",
    "        new_key = new_key.replace(\"encoder_attn.out_proj.weight\", \"cross_attn.out.weight\")\n",
    "        new_key = new_key.replace(\"encoder_attn.out_proj.bias\",   \"cross_attn.out.bias\")\n",
    "        new_key = new_key.replace(\"encoder_attn_layer_norm.weight\", \"cross_attn_ln.weight\")\n",
    "        new_key = new_key.replace(\"encoder_attn_layer_norm.bias\",   \"cross_attn_ln.bias\")\n",
    "\n",
    "        # 6) MLP (feed-forward)\n",
    "        new_key = new_key.replace(\"fc1.weight\", \"mlp.0.weight\")\n",
    "        new_key = new_key.replace(\"fc1.bias\",   \"mlp.0.bias\")\n",
    "        new_key = new_key.replace(\"fc2.weight\", \"mlp.2.weight\")\n",
    "        new_key = new_key.replace(\"fc2.bias\",   \"mlp.2.bias\")\n",
    "\n",
    "        # 7) Old layer norms -> attn_ln, mlp_ln\n",
    "        #    HF: \"self_attn_layer_norm\" -> \"ln_attn\"\n",
    "        #    OAI: \"attn_ln\"\n",
    "        #    HF: \"final_layer_norm\" -> \"ln_mlp\"\n",
    "        #    OAI: \"mlp_ln\"\n",
    "        new_key = new_key.replace(\"self_attn_layer_norm.weight\", \"attn_ln.weight\")\n",
    "        new_key = new_key.replace(\"self_attn_layer_norm.bias\",   \"attn_ln.bias\")\n",
    "        new_key = new_key.replace(\"final_layer_norm.weight\",     \"mlp_ln.weight\")\n",
    "        new_key = new_key.replace(\"final_layer_norm.bias\",       \"mlp_ln.bias\")\n",
    "\n",
    "        # 8) If you also used \"ln_attn\"/\"ln_mlp\" previously, rename them to OAI's \"attn_ln\", \"mlp_ln\"\n",
    "        new_key = new_key.replace(\"ln_attn\", \"attn_ln\")\n",
    "        new_key = new_key.replace(\"ln_mlp\",  \"mlp_ln\")\n",
    "\n",
    "        # 9) The top-level LN for the encoder/decoder is \"encoder.ln_post\" / \"decoder.ln_post\"\n",
    "        #    That might still be okay. If official code expects \"encoder.ln_post\" we can keep it.\n",
    "        #    Just ensure there's no mismatch, e.g. \"layer_norm\" -> \"ln_post\"\n",
    "        new_key = new_key.replace(\"encoder.layer_norm.weight\", \"encoder.ln_post.weight\")\n",
    "        new_key = new_key.replace(\"encoder.layer_norm.bias\",   \"encoder.ln_post.bias\")\n",
    "        new_key = new_key.replace(\"decoder.layer_norm.weight\", \"decoder.ln_post.weight\")\n",
    "        new_key = new_key.replace(\"decoder.layer_norm.bias\",   \"decoder.ln_post.bias\")\n",
    "\n",
    "        new_key = new_key.replace(\"decoder.ln_post.\", \"decoder.ln.\")\n",
    "\n",
    "        return new_key\n",
    "\n",
    "    for old_key, val in hf_sd.items():\n",
    "        new_key = rename_key(old_key)\n",
    "        openai_sd[new_key] = val\n",
    "\n",
    "    # Remove or rename 'proj_out.weight' if your code doesn't expect it at all:\n",
    "    if \"proj_out.weight\" in openai_sd:\n",
    "        # Option A: just drop it:\n",
    "        openai_sd.pop(\"proj_out.weight\")\n",
    "\n",
    "        # Option B: rename it to something your code expects, e.g. \"decoder.proj.weight\"\n",
    "        #   openai_sd[\"decoder.proj.weight\"] = openai_sd.pop(\"proj_out.weight\")\n",
    "\n",
    "\n",
    "    # For \"tiny\", provide the official dims from whisper/model.py\n",
    "    if model_size == \"tiny\":\n",
    "      dims = {\n",
    "          \"n_mels\": 80,\n",
    "          \"n_audio_ctx\": 1500,\n",
    "          \"n_audio_state\": 384,\n",
    "          \"n_audio_head\": 6,\n",
    "          \"n_audio_layer\": 4,   # 4 encoder layers\n",
    "          \"n_vocab\": hf_model.config.vocab_size,\n",
    "          \"n_text_ctx\": 448,\n",
    "          \"n_text_state\": 384,\n",
    "          \"n_text_head\": 6,\n",
    "          \"n_text_layer\": 4,    # 4 decoder layers\n",
    "      }\n",
    "    elif model_size == \"large-v2\":\n",
    "      dims = {\n",
    "          \"n_mels\": 80,\n",
    "          \"n_audio_ctx\": 1500,\n",
    "          \"n_audio_state\": 1280,\n",
    "          \"n_audio_head\": 20,\n",
    "          \"n_audio_layer\": 32,\n",
    "          \"n_vocab\": hf_model.config.vocab_size,\n",
    "          \"n_text_ctx\": 448,\n",
    "          \"n_text_state\": 1280,\n",
    "          \"n_text_head\": 20,\n",
    "          \"n_text_layer\": 32,\n",
    "      }\n",
    "\n",
    "    return {\n",
    "        \"model_state_dict\": openai_sd,\n",
    "        \"dims\": dims,\n",
    "        \"name\": model_size,\n",
    "        # If it's the .en variant, set this to False; otherwise True\n",
    "        \"multilingual\": True\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/spiral_hdd_2/workspace/naren/models/whisper-large-v2-pa-5\"\n",
    "best_checkpoint_path = \"/spiral_hdd_2/workspace/naren/models/whisper-large-v2-pa-5/checkpoint-best\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "print(\"Converting HF model to OpenAI-like format...\")\n",
    "whisper_model = convert_hf_to_openai_checkpoint(model, model_size=\"large-v2\")\n",
    "\n",
    "# Save the converted checkpoint\n",
    "converted_ckpt_path = f\"{checkpoint_path}/best_checkpoint.pt\"\n",
    "torch.save(whisper_model, converted_ckpt_path)\n",
    "print(f\"Saved converted checkpoint to {converted_ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_path = data_path / \"Audio\" / \"AniBook Videos\" / \"Abdul_Kalam,_Missile_Man_Punjabi.wav\"\n",
    "\n",
    "test_audio, sampling_rate = sf.read(test_audio_path)\n",
    "# test_input_features = processor(test_audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the srt file and convert to text and store it\n",
    "import pysrt\n",
    "\n",
    "test_srt_path = data_path / \"Ground Truth SRT\" / \"AniBook SRT\" / \"Abdul_Kalam,_Missile_Man_Punjabi.srt\"\n",
    "\n",
    "srt_file = pysrt.open(test_srt_path)\n",
    "for subtitle in srt_file:\n",
    "    print(f\"Start: {subtitle.start}, End: {subtitle.end}, Text: {subtitle.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper Load model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "converted_ckpt_path = \"/spiral_hdd_2/workspace/naren/models/whisper-large-v2-pa-5/best_checkpoint.pt\"\n",
    "whisper_model = whisper.load_model(converted_ckpt_path)\n",
    "# whisper_model = whisper.load_model(\"large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_audio = test_audio.astype(np.float32)\n",
    "total_duration = len(test_audio) / sampling_rate\n",
    "print(f\"Total duration of the audio file: {total_duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clip timestamps at 10 second intervals, with each timestamp repeated\n",
    "# to create pairs for start/end times up to 290 seconds\n",
    "# clip_timestamps = []\n",
    "# for i in range(0, 290, 10):\n",
    "#     clip_timestamps.extend([i, i+10])\n",
    "# print(clip_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transcribe using whisper model\n",
    "initial_prompt = \"ਸਤ ਸ੍ਰੀ ਅਕਾਲ, ਇਹ ਇੱਕ ਪੰਜਾਬੀ ਆਡੀਓ ਫਾਇਲ ਹੈ, ਇਸ ਦਾ ਸਮੱਗਰੀ ਹੇਠ ਲਿਖੀ ਹੈ।\"\n",
    "# clip_timestamps = \"0,10,10,20,30\"\n",
    "# total duration of the audio file\n",
    "\n",
    "# result = whisper_model.transcribe(test_audio, language=\"pa\", word_timestamps=True, initial_prompt=initial_prompt, clip_timestamps=clip_timestamps)\n",
    "result = whisper_model.transcribe(test_audio, language=\"pa\", word_timestamps=True, initial_prompt=initial_prompt)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_transcript = result[\"text\"]\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for segment in result[\"segments\"]:\n",
    "    print(f\"Start: {segment['start']}, End: {segment['end']}, Text: {segment['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_text_path = \"/home/arjun/naren/alignment/data/Punjabi/Text/AniBook Text/Abdul_Kalam,_Missile_Man_Punjabi.txt\"\n",
    "with open(GT_text_path, \"r\") as f:\n",
    "    gt_text = f.read()\n",
    "\n",
    "gt_transcript = \" \".join(gt_text.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "measures = jiwer.compute_measures(gt_transcript, pred_transcript)\n",
    "print(f\"WER: {measures['wer']}\")\n",
    "print(f\"Insertions: {measures['insertions']}\")\n",
    "print(f\"Deletions: {measures['deletions']}\")\n",
    "print(f\"Substitutions: {measures['substitutions']}\")\n",
    "print(f\"Ground Truth Total Words: {len(gt_transcript.split())}\")\n",
    "print(f\"Predicted Total Words: {len(pred_transcript.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import whisper\n",
    "\n",
    "# Load your Whisper model\n",
    "whisper_model = whisper.load_model(\"large-v2\")\n",
    "\n",
    "# Load your audio file using PyDub\n",
    "audio = AudioSegment.from_file(test_audio_path)\n",
    "\n",
    "# Define your desired chunk duration in milliseconds (e.g., 1 minute)\n",
    "chunk_duration_ms = 10000  # 10 seconds\n",
    "chunk_duration_ms_overlap = 13000\n",
    "chunks = [audio[i:i + chunk_duration_ms_overlap] for i in range(0, len(audio), chunk_duration_ms)]\n",
    "\n",
    "# Convert each chunk to numpy array\n",
    "def audio_to_numpy(audio_segment):\n",
    "    samples = np.array(audio_segment.get_array_of_samples())\n",
    "    return samples.astype(np.float32) / (2 ** 15)  # Normalize to [-1, 1]\n",
    "\n",
    "chunk_arrays = [audio_to_numpy(chunk) for chunk in chunks]\n",
    "\n",
    "# Transcribe each chunk\n",
    "outputs = []\n",
    "initial_prompt = \"ਸਤ ਸ੍ਰੀ ਅਕਾਲ, ਇਹ ਇੱਕ ਪੰਜਾਬੀ ਆਡੀਓ ਫਾਇਲ ਹੈ, ਇਸ ਦਾ ਸਮੱਗਰੀ ਹੇਠ ਲਿਖੀ ਹੈ۔\"\n",
    "for i, chunk in enumerate(chunk_arrays):\n",
    "    result = whisper_model.transcribe(chunk, language=\"pa\", word_timestamps=True, initial_prompt=initial_prompt)\n",
    "    outputs.append(result)\n",
    "    for segment in result[\"segments\"]:\n",
    "        print(f\"Start: {segment['start']}, End: {segment['end']}, Text: {segment['text']}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    for segment in output[\"segments\"]:\n",
    "        print(f\"Start: {segment['start']}, End: {segment['end']}, Text: {segment['text']}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path / \"Text\" / \"AniBook Text\" / \"Abdul_Kalam,_Missile_Man_Punjabi.txt\", \"r\") as f:\n",
    "    gt_text = f.read()\n",
    "    print(gt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_transcript = result[\"text\"]\n",
    "gt_transcript = \" \".join(gt_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_transcript = \"ਮਿਜ਼ਾਇਲ ਮੈਂ ਅਦਣੀ ਮਿਜ਼ਾਇਲ ਨੂੰ ਲੰਜ ਕਰਨ ਵਿੱਚ ਹੋ ਰਹੀਆਂ ਵਾਰਵਾਰ ਅਸਫ਼ਲਦਾਵਾਂ ਅਤੇ ਦੇਰੀ ਨੇ ਪਰੈਸ ਵਿੱਚ ਕਾਫ਼ੀ ਗੱਲਾਂ ਕਰਵਾ ਦਿੱਤੀਆਂ ਸਨ ਰਾਕੇਟ ਵਿਗਿਆਨ ਵਿੱਚ ਅਜੇ ਹੀਆਂ ਦੇਰੀਆਂ ਆਮ ਹੈਂ ਪਰ ਦੇਸ਼ ਸਮਝਣ ਦੇ ਮੋੜ ਵਿੱਚ ਨਹੀਂ ਸੀ ਸਾਡੀਆਂ ਮੁਸ਼ਕਿਲਾਂ ਸਿਰਫ਼ ਸਾਡੀਆਂ ਨੇ ਇਹਨਾਂ ਦੇਰਿਆਂ ਦਾ ਆਪਣੇ ਅਨੂਸਾਰ ਮਤਲਬ ਕੱਢੇ ਲਿਆ ਸੀ ਹਤੇ ਕੁੱਝ ਅਨੋਖੇ ਵਿਕਲਬ ਸੁਝਾਏ। ਅਮੂਲ ਨੇ ਇਹ ਵੀ ਇੱਕ ਘਾਟੂਣ ਬਣਾਇਆ ਸੀ ਜਿਸ ਵਿੱਚ ਸੁਝਾਓ ਦਿੱਤਾ ਸੀ ਕਿ ਅਗਨੀ ਨੂੰ ਇਹਦਨ ਵੱਜੋਂ ਉਨਾਂ ਦਾ ਮੱਖਣ ਵਰਤਣਾ ਚਾਹੀਦਾ ਹੈ ਪਰਿਸ ਦੇਰੀ ਘਾਰਨ ਗੁੱਸੇ ਵਿੱਚ ਸੀ ਅਤੇ ਮੋਰੀ ਤਰ੍ਹਾਂ ਨਰਾਸ਼ ਹੋ ਗਏ ਸੀ। ਮੈਨੂੰ ਕਹੀ ਮਾਕੇ ਯਾਦ ਆਏ ਜਦੋਂ ਮੇਰੀ ਟੀਮ ਦੇ ਲੀਡਰਾਂ ਨੇ ਸਾਡਾ ਮਨੋਬਲੋ ਅਧਾਇਆ ਸੀ ਅਦੇ ਮੈਂ ਵੀ ਅਜਿਹਾ ਕਰਨ ਬਾਰੇ ਸੋਚਿਆ ਮੈਂ ਇੱਕ ਮੀਟਿੰਗ ਲਾਈ ਅਤੇ ਦੋ ਹਜ਼ਾਰ ਮੈਂਬਰਾਂ ਦੀ ਆਪਣੀ ਟੀਮ ਨੂੰ ਸਮੋਦਿਤ ਕੀਤਾ ਸਾਨੂੰ ਇੱਕ ਵਧੀਆ ਮਾਕਾ ਦਿੱਤਾ ਗਨਾਲ ਉਹਨੀਆਂ ਹੀ ਵੱਡੀਆਂ ਚਾਣਾਉਤੀਆਂ ਵੀ ਆਉਂਦੀਆਂ ਹੈਂ, ਅਸੀਂ ਹਾਰ ਨਹੀਂ ਮੰਨ ਸਕਦੇ, ਸਾਡਾ ਦੇਸ਼ ਸਾਡੇ ਵੱਲੋਂ ਕਿਸੀ ਵੀ ਚੀਜ਼ ਦੀ ਕਮੀ ਦਾ ਹੱਕਦਾਰ ਨਹੀਂ ਹੈ। ਮੈਂ ਆਪਣੇ ਲੋਕਾਂ ਨੂੰ ਇਹ ਕਹਿ ਕੇ ਆਪਣੀ ਗੱਲ ਖ਼ਤਮ ਕੀਤੀ, ਮੈਂ ਤੁਹਾਨੂੰ ਵਾਦਾ ਕਰਦਾ ਹਾਂ, ਕਿ ਜਾਡੇ ਅਗਲੇ ਟਰਾਇਲ ਵਿੇਰੀ ਟੀਮ ਨੂੰ ਆਪਣੀ ਉਰਜਾ ਮੁੜ ਮਿਲ ਗਈ ਸੀ, ਆਪਣੇ ਜੋਸ਼ ਨੂੰ ਮੁੜ ਤਾਜਾ ਕਰਦਿਆਂ ਹੁਣਾਂ ਨੇ ਬਹੁਤ ਧਿਆਨ ਲੱਗਾ ਕਿ ਅਤੇ ਇੱਛਾ ਸ਼ਕਤੀ ਨਾਲ ਕੰਮ ਕੀਤਾ। ਆਖਰ ਘਾਰ ਬਾਈ ਮਾਈ ਉੱਨੀ ਸੋਂਰਾਨਵੇਂ ਨੂੰ ਲੰਜ ਦਹਿ ਕਰ ਦਿੱਦਾ ਗਿਆ ਸੀ। ਆਰਮੀ ਸਟਾਫ਼ ਤੇ ਮੁੱਖੀ ਅਤੇ ਰੱਖਿਆ ਮੰਦਰੀ ਵਰਗੇ ਵੱਛਲੀ ਰਾਤ ਸਾਡੇ ਵਿਚੋਂ ਕੁੱਝ ਲੋਕ ਸਮੁੰਦਰੀ ਕੰਢੇ ਉੱਤੇ ਸਹਿਰ ਕਰਨ ਗਏ ਹੋਏ ਸਨ ਜੇ ਉਹ ਪੂਰੇ ਜਨ ਨਾਲ ਜਗਮਗਾ ਰਿਹਾ ਸੀ ਲਹਿਰਾਂ ਗਰਜ਼ ਕਿਉਠੀਆਂ ਅਤੇ ਚੱਟਾਂ ਨਾਲ ਉੱਤੇ ਆ ਕੇ ਡਿੱਗੀਆਂ ਕਿ ਅਸੀਂ ਕੱਲ੍ਹ ਅਗਣੀ ਲਾਂਜ ਕਰਨ ਵਿੱਚ ਸਫ਼ਲ ਹੋਵਾਂਗੇ ਇਹ ਸਵਾਲ ਹਰ ਕਿਸੇ ਦੇ ਦਤਿਆਰ ਨਹੀਂ ਸੀ। ਆਖਰ ਘਾਰ ਰੱਖਿਆ ਮਨਤਰੀ ਨੇ ਚੁੱਪੀ ਤੋੜੀ ਅਤੇ ਮੈਨੂੰ ਪੁੱਛਿਆ। ਕਲਾਮ ਕੱਲ੍ਹ ਅਸੀਂ ਅਗਣੀ ਦੀ ਸਫ਼ਲ ਦਾ ਕਿਵੇਂ ਮਨਾਈਏ ਕਿ ਤੁਹਾਡੇ ਦਿਲਦੀ ਕੋਈ ਇੱਛਾ ਹੈ ਇਹ ਇੱਕ ਸਰਲ ਸਵਾਲ�ਂ ਸੀ? ਫਿਰ ਮੈਂ ਇਹਨੂੰ ਜਵਾਬ ਮਿਲਿਆ। ਸਾਨੂੰ ਸਾਡੇ ਖੁੱਝ ਕੇ ਇੰਦਰ ਵਿੱਚ ਲਗਾਉਣ ਲਈ ਇੱਕ ਇਲੱਕ ਕਿਪੂਟੇ ਚਾਹੀਦੇ ਹਨ। ਮੈਂ ਕਿਹਾ ਮਨਤਰੀ ਜੀ ਦਾ ਚਿਹਰਾ ਦੋਸਤਾ ਨਾ ਚਮਕ ਨਾਲ ਚਮਕ ਉਠਿਆ ਤੁਸੀਂ ਅਗਣੀ ਲਈ ਤਰਤੀ ਮਾਂ ਦਾ ਅਸ਼ੀਰਤੇ ਅਗਨੀ ਲਈ ਤਰਤੀਮਾਂ ਦਾ ਅਸ਼੍ਰਵਾਰ ਲੈ ਰਹੇ ਹੋ। ਅਸੀਂ ਕੱਲ੍ਹ ਜ਼ਰੂਲ ਸਫ਼ਲ ਹੋਵਾਂਗੇ, ਹੁਣਾਂ ਨੇ ਨਿੱਕੀ ਪਵਿਕਵਾਣੀ ਕੀਤੀ। ਅਗਲੇ ਦਿਨ ਸਵੇਰ ਇਹ ਸੱਤ ਦੱਸ ਦੇ ਅਗਨੀ ਨੇ ਉਡਾਨ ਪਰੀ, ਇਹ ਬਿਲਕੁਲ ਸਹੀ ਲਾਂਜ ਸੀ। ਇਹ ਰਾਤ ਦੀ ਡਰਾਉਣੇ ਨੀਨ ਤੋਂ ਬਾਹਦ, ਉੱਜਵਾਲ ਅਤੇ ਤਾਜੀ ਸਵੇਰ ਵਿੱਖਵੱਖ ਕਾਰਜ ਕਿੰਨਰਾਂ ਤੇ ਪੰਜ ਸਾਲਾਂ ਦੇ ਨਿਰਨ ਤਰ ਕੰਮ ਦੇ ਨਤੀਜੇ ਦਾ ਫਲ ਆਖਰ ਕਾਰ ਮਿਲ ਗਿਆ ਸੀ। ਸਿਰਫ਼ ਛੇਸੋ ਸਕਿੰਟਾਂ ਦੀ ਸ਼ਾਨਦਾਰ ਰੁਡਾਨ ਨੇ ਸਾਡੀ ਸਾਰੀ ਥਕਾਵੱਟ ਦੂਰ ਕਰ ਦਿੱਤੀ ਸੀ। ਕਈ ਸਾਲਾਂ ਦੀ ਮਿਹਨਾਂ ਦਾ ਇੰਨਾਂ ਸ਼ਾਨਦਾਰ ਫਲ ਮਿਲਿਆ ਸੀ। ਇਹ ਮੇਰੀ ਜ਼਼ਾਨਗੀ ਦੇ ਸਭ ਤੋਂ ਮਾਹਾਨ ਪਲਾ ਵਿੱਚੋਂ ਇੱਕ ਸੀ ਅਤੇ ਰਹੇਗਾ।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[pred_transcript], references=[gt_transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "# compute the number of insertions, deletions, substitutions\n",
    "measures = jiwer.compute_measures(gt_transcript, pred_transcript)\n",
    "print(f\"WER: {measures['wer']}\")\n",
    "print(f\"Insertions: {measures['insertions']}\")\n",
    "print(f\"Deletions: {measures['deletions']}\")\n",
    "print(f\"Substitutions: {measures['substitutions']}\")\n",
    "print(f\"Ground Truth Total Words: {len(gt_transcript.split())}\")\n",
    "print(f\"Predicted Total Words: {len(pred_transcript.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> List[str]:\n",
    "    text = \" \".join([word.strip() for word in text.strip().split()])\n",
    "    # remove all punctuations including brackets\n",
    "    text = re.sub(r'[.,।?!♪”‘॥\\…\\-\\(\\)\\[\\]\\{\\}]', '', text)\n",
    "    # text = re.sub(r'[^\\w\\s]|♪', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_text_in_brackets(text: str) -> str:\n",
    "    \"\"\"Remove all text between square brackets, including the brackets.\n",
    "    \n",
    "    This function will remove any text that is enclosed within square brackets \n",
    "    (e.g., [example], [text to remove], [1, 2, 3], etc.) along with the brackets themselves.\n",
    "    It will also handle nested brackets and any text that may appear before or after the brackets.\n",
    "    \"\"\"\n",
    "    # Use a stack to handle nested brackets\n",
    "    stack = []\n",
    "    result = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "        if text[i] == '[':\n",
    "            stack.append(i)  # Push the index of the opening bracket onto the stack\n",
    "        elif text[i] == ']' and stack:\n",
    "            stack.pop()  # Pop the last opening bracket index\n",
    "            if not stack:  # If the stack is empty, it means we found a complete bracket pair\n",
    "                # Remove the text between the last opening and this closing bracket\n",
    "                result.append('')  # Append an empty string to remove the content\n",
    "        else:\n",
    "            if not stack:  # Only add characters to result if we are not inside brackets\n",
    "                result.append(text[i])\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(result).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" Text\"\n",
    "srt = \" SRT\"\n",
    "vid = \" Videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    text = \" \".join([word.strip() for word in text.strip().split()])\n",
    "    # remove punctuations\n",
    "    # text = re.sub(r'[.,।?!-]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_transcripts = {}\n",
    "filenames = []\n",
    "segment_paths = data_path / \"Results\" / \"Segments_26_01\"\n",
    "for dir_path in transcript_path.glob(\"*\"):\n",
    "    for file_path in dir_path.glob(\"*.txt\"):\n",
    "        if any(benchmark_name in file_path.name for benchmark_name in benchmark_names):\n",
    "            print(file_path.name)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                gt_transcript = f.read()\n",
    "            gt_transcript = preprocess_text(gt_transcript)\n",
    "            gt_transcripts[file_path.name] = gt_transcript\n",
    "            filenames.append(file_path.name.replace(\".txt\", \"\"))\n",
    "\n",
    "filenames = sorted(filenames)\n",
    "\n",
    "pred_transcripts = {}\n",
    "\n",
    "print(\"-\"*100)\n",
    "for file_path in segment_paths.glob(\"*.json\"):\n",
    "    if any(benchmark_name in file_path.name for benchmark_name in benchmark_names):\n",
    "        print(file_path.name)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "        pred_transcripts[file_path.name] = preprocess_text(result[\"text\"])\n",
    "\n",
    "print(\"-\"*100)\n",
    "jiwer_df = pd.DataFrame()\n",
    "jiwer_df[\"Base Name\"] = filenames\n",
    "jiwer_df[\"WER\"] = float(\"nan\")\n",
    "jiwer_df[\"Insertions\"] = 0\n",
    "jiwer_df[\"Deletions\"] = 0\n",
    "jiwer_df[\"Substitutions\"] = 0\n",
    "jiwer_df[\"Predicted Total Words\"] = 0\n",
    "\n",
    "for base_name in sorted(filenames):\n",
    "    gt_transcript = gt_transcripts[base_name + \".txt\"]\n",
    "    gt_transcript = remove_text_in_brackets(gt_transcript)\n",
    "    gt_transcript = preprocess_text(gt_transcript)\n",
    "    pred_transcript = pred_transcripts[base_name + \".json\"]\n",
    "    pred_transcript = preprocess_text(pred_transcript)\n",
    "    measures = jiwer.compute_measures(gt_transcript, pred_transcript)\n",
    "    print(f\"Base Name: {base_name}\")\n",
    "    print(f\"WER: {measures['wer']}\")\n",
    "    print(f\"Insertions: {measures['insertions']}\")\n",
    "    print(f\"Deletions: {measures['deletions']}\")\n",
    "    print(f\"Substitutions: {measures['substitutions']}\")\n",
    "    print(f\"Ground Truth Total Words: {len(gt_transcript.split())}\")\n",
    "    print(f\"Predicted Total Words: {len(pred_transcript.split())}\")\n",
    "    print(\"-\"*100)\n",
    "    # round the wer value to 2 decimal places\n",
    "    jiwer_df.loc[jiwer_df[\"Base Name\"] == base_name, \"WER\"] = round(measures[\"wer\"]*100, 2)\n",
    "    jiwer_df.loc[jiwer_df[\"Base Name\"] == base_name, \"Insertions\"] = measures[\"insertions\"]\n",
    "    jiwer_df.loc[jiwer_df[\"Base Name\"] == base_name, \"Deletions\"] = measures[\"deletions\"]\n",
    "    jiwer_df.loc[jiwer_df[\"Base Name\"] == base_name, \"Substitutions\"] = measures[\"substitutions\"]\n",
    "    jiwer_df.loc[jiwer_df[\"Base Name\"] == base_name, \"Predicted Total Words\"] = len(pred_transcript.split())\n",
    "    jiwer_df.loc[jiwer_df[\"Base Name\"] == base_name, \"GT Total Words\"] = len(gt_transcript.split())\n",
    "\n",
    "jiwer_df.to_csv(data_path / \"Results\" / \"Jiwer_results.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiwer_df.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1Bdiwk8-cnH3nJBD6ENDcZfwFmrgZDtrM",
     "timestamp": 1735705079208
    }
   ]
  },
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3a64fdddd0624bc094455c516e5cacef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3eb8ab26a45b48b39585faed17039664": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4087853866024331a9a065fbdb722c2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41fd39d7ffc44620925de7683092419a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f33c9f02c3a42fd98e1660cc095eae8",
      "placeholder": "​",
      "style": "IPY_MODEL_649fb3b81fb64780b64f7706f18fbb78",
      "value": "Map: 100%"
     }
    },
    "4f33c9f02c3a42fd98e1660cc095eae8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "649fb3b81fb64780b64f7706f18fbb78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68546b13fe7e4ae2b17ce18907ab6e64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f722f8fe8b524e289bf9a36eee23f263",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_87a1abe581f048aaa02be13795f3e8d4",
      "value": 2
     }
    },
    "87a1abe581f048aaa02be13795f3e8d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6744e5c5ec14cd6a10dd32480ec25d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_41fd39d7ffc44620925de7683092419a",
       "IPY_MODEL_68546b13fe7e4ae2b17ce18907ab6e64",
       "IPY_MODEL_d78bf484c902455bb0c6e47017e81e3a"
      ],
      "layout": "IPY_MODEL_3eb8ab26a45b48b39585faed17039664"
     }
    },
    "d78bf484c902455bb0c6e47017e81e3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4087853866024331a9a065fbdb722c2d",
      "placeholder": "​",
      "style": "IPY_MODEL_3a64fdddd0624bc094455c516e5cacef",
      "value": " 2/2 [00:05&lt;00:00,  2.34s/ examples]"
     }
    },
    "f722f8fe8b524e289bf9a36eee23f263": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
